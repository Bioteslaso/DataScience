{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6462427d-f93a-46a3-aef1-f16e7a7f7bbd",
   "metadata": {},
   "source": [
    "# Questions on Naïve Bayes and k-Nearest Neighbors (kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0c2c3-3e22-4597-af3a-71beca5eede0",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee0b18-14aa-477b-bbf2-fb5c03b7abec",
   "metadata": {},
   "source": [
    "Assume that we want to use a Naïve Bayes classifier on a binary\n",
    "classification task, with the class labels being $c1$ and $c2$ and\n",
    "involving the binary features $f1$ and $f2$.  Moreover, asume a\n",
    "uniform class prior, i.e, $P(c1) = P(c2)$ and that the class\n",
    "conditional probabilities include $P(f1=0|c1) = 0$ and $P(f2=0|c2) = 1$.\n",
    "\n",
    "What class label $c \\in \\{c1,c2\\}$ maximizes $P(c|f1=0 \\& f2=1)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3f7d0-cae5-4ec6-8f3f-487203568c54",
   "metadata": {},
   "source": [
    "A: c1 will get a higher probability than c2\n",
    "\n",
    "B: c2 will get a higher probability than c1\n",
    "\n",
    "C: c1 and c2 will get the same probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa350e1-6fde-40a1-9653-bfbcee99adf3",
   "metadata": {},
   "source": [
    "Correct answer: C\n",
    "\n",
    "Explanation: When calculating $P(c1|f1=0 \\& f2=1)$ using Naïve Bayes, the class prior $P(c1)$ is multiplied with $P(f1=0|c1)$ and $P(f2=1|c1)$, and divided by $P(f1=0 \\& f2=1)$. Since $P(f1=0|c1) = 0$, it means that according to Naïve Bayes, $P(c1|f1=0 \\& f2=1) = 0$.\n",
    "\n",
    "When calculating $P(c2|f1=0 \\& f2=1)$ using Naïve Bayes, the class prior $P(c2)$ is multiplied with $P(f1=0|c2)$ and $P(f2=1|c2)$, and divided by $P(f1=0 \\& f2=1)$. Since $P(f2=1|c2) = 1 - P(f2=0|c2) = 0$, it means that according to Naïve Bayes, $P(c2|f1=0 \\& f2=1) = 0$.\n",
    "\n",
    "In other words, Naïve Bayes would assign a probability of zero for both class labels, and hence both\n",
    "labels would maximize the expression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e7087-3555-4536-b621-0515ed07b2cb",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd8e3c-8e2f-465d-b1f3-6d6bd466def0",
   "metadata": {},
   "source": [
    "Assume that we are facing a binary classification task, where a positive class label ($+$) is observed when the binary features $f_1$ and $f_2$ both have a value of 0 or 1, and a negative label ($-$) is observed in all other cases, i.e., when $f_1 \\neq f_2$. \n",
    "\n",
    "Can Naïve Bayes be expected to learn an accurate model for this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f08207-06e2-4dbf-aa35-3845df086260",
   "metadata": {},
   "source": [
    "A: Yes\n",
    "\n",
    "B: No\n",
    "\n",
    "C: Maybe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638440a-1c5f-4ef8-a5c0-8e6b6605e12f",
   "metadata": {},
   "source": [
    "Correct answer: No\n",
    "\n",
    "Explanation: \n",
    "\n",
    "From the description of the classification task, it follows that $P(+|f_1=0 \\And f_2=0)$ and $P(+|f_1=1 \\And f_2=1)$ should be high (ideally 1), and $P(-|f_1=0 \\And f_2=0)$ and $P(-|f_1=1 \\And f_2=1)$ should be low (ideally 0), while $P(+|f_1=1 \\And f_2=0)$ and $P(+|f_1=0 \\And f_2=1)$ should be low, and $P(-|f_1=1 \\And f_2=0)$ and $P(-|f_1=0 \\And f_2=1)$ should be high.\n",
    "\n",
    "According to Bayes' theorem, the above can be calculated from the class priors, $P(+)$ and $P(-)$, and conditional probabilities, e.g., $P(f_1=0 \\And f_2=0|+)$. \n",
    "\n",
    "In Naïve Bayes, the conditional probabilities are broken up, e.g., $P(f_1=0 \\And f_2=0|+)$ is assumed to be equivalent to the product of $P(f_1=0|+)$ and $P(f_2=0|+)$, i.e., the two events ($f_1=0 \\And f_2=0$) are assumed to be independent given the class ($+$). This assumption is clearly violated here, since if one of the events is known, e.g., $f_1=0$, the probability of the other event is clearly affected, e.g., the probability of $f_2=0$ is high, given the class $+$. \n",
    "\n",
    "If we would assume that all combinations of feature values are equally likely, and provide labels for a (complete) training set according to the description, both class priors will be the same (0.5) as well as all conditional probabilities employed by Naïve Bayes, i.e., $P(f_1=1|+) = P(f_1=0|+) = P(f_2=1|+) = P(f_2=0|+) = 0.5$ and $P(f_1=1|-) = P(f_1=0|-) = P(f_2=1|-) = P(f_2=0|-) = 0.5$. This means that Naïve Bayes will output the same class probabilities independently of what instance is being classified, hence clearly not being able to discriminate between the two classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623cc6c-e3ed-432b-b6a8-43ece77d34c9",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730b20b-5366-4ba5-a5f4-82e6171522f0",
   "metadata": {},
   "source": [
    "Assume that a large number of binary features are added to a dataset\n",
    "with two class labels c1 and c2, such that for each added feature f,\n",
    "the class conditional probability $P(f=0|c1) = P(f=0|c2)$. What\n",
    "potential effect will the addition of such features have on the\n",
    "accuracy of Naïve Bayes and kNN respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab7f50-f79d-43aa-9906-960c4dbda555",
   "metadata": {},
   "source": [
    "A This will not have any effect on any of the algorithms\n",
    "\n",
    "B This will have an effect on Naïve Bayes only\n",
    "\n",
    "C This will have an effect on k-Nearest Neighbors only\n",
    "\n",
    "D This will have an effect on both algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab669457-d1fd-421f-bfd9-f059b4122d4d",
   "metadata": {},
   "source": [
    "Correct answer: C\n",
    "\n",
    "Explanation: When looking for the class label $c$ that maximizes $P(c|f1=v1, \\ldots, fn=vn)$ for the features $f1, \\ldots, fn$ and values $v1, \\ldots, vn$, using Naïve Bayes, the class prior $P(c)$ is multiplied with $\\prod_{i=1}^{n}P(fi=vi|c)$.\n",
    "\n",
    "Hence, all features for which $P(f=0|c1) = P(f=0|c2)$ (and hence $P(f=1|c1) = P(f=1|c2)$, since the features are binary and the probabilities sum to one) will have no effect on the predicted class probabilities, and hence the accuracy will not be affected.\n",
    "\n",
    "However, for kNN the added features will be taken into account in the distance calculations, possibly changing the neighborhood of the test instances, and hence also result in different predictions, which may have a (positive or negative) effect on the accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee2962f-dab7-4ccc-a1cd-8d613af132a2",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1224e-7e5d-4544-827d-f623354782cb",
   "metadata": {},
   "source": [
    "Assume that for one feature there is a large number of missing values, while the non-missing values are all identical. May the k-nearest neighbor algorithm using the Euclidean distance be affected if we chose to remove the feature completely instead of imputing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5522d4-1135-444a-8e9e-25870ad076f9",
   "metadata": {},
   "source": [
    "A: Yes and this holds independently of how we chose to do the imputation\n",
    "\n",
    "B: No, we will always get the same results\n",
    "\n",
    "C: The results will be the same if we chose to impute missing values with the mean of the non-missing\n",
    "\n",
    "D: The previous (C) would hold also even if the non-missing values are not identical "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7705aa-bb38-41ec-ba8f-11f4be19c71d",
   "metadata": {},
   "source": [
    "Correct answer: C\n",
    "\n",
    "Explanation: If all the instances have the same value, then keeping the feature will result in that the same term is added to all the distance calculations; the relative distances will hence not be affected if removing the feature. This does not hold if the instances have more than one unique value, which is the case for alternative D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2adaf2-2a35-4419-8be8-bceda2bfcc5d",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c75a9-712e-4d15-8b13-805e5ccca8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which of the following will be useful for finding the nearest neighbors \n",
    "# to a test object x_test in a set of instances X_train?\n",
    "\n",
    "# A\n",
    "A = sorted([distance(x_test,x_train) for x_train in X_train])\n",
    "\n",
    "# B\n",
    "B = sorted([(i, distance(x_test,x_train)) for i, x_train in enumerate(X_train)])\n",
    "\n",
    "# C\n",
    "C = sorted([(distance(x_test,x_train),i) for i, x_train in enumerate(X_train)])\n",
    "\n",
    "# D\n",
    "D = np.argsort([distance(x_test,x_train) for x_train in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f7b23-3d69-4aa0-8c58-c2977bd55356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct answers: C and D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c5d30-0007-4b65-a251-0f27ae52bcc3",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4045ec5f-b200-4551-af27-376b57cb5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"CLASS\":[\"a\",\"b\",\"c\",\"c\",\"b\"], \n",
    "                   \"F1\":[1,1,2,2,3],\n",
    "                   \"F2\":[2,2,4,4,2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1dc13c55-b6b1-4efe-af78-80c32e393af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which of the following will generate a dictionary with a mapping from the\n",
    "# class labels to the relative frequencies?\n",
    "\n",
    "# A\n",
    "a = {}\n",
    "for c in df[\"CLASS\"].astype(\"category\").cat.categories:\n",
    "    a[c] = sum(df[\"CLASS\"]==c)/len(df)\n",
    "\n",
    "# B\n",
    "b = {}\n",
    "for c, g in df.groupby(\"CLASS\"):\n",
    "    b[c] = len(g)/len(df)\n",
    "\n",
    "# C\n",
    "c = {c:sum(df[\"CLASS\"]==c)/len(df) for c in pd.unique(df[\"CLASS\"])}\n",
    "\n",
    "# D\n",
    "d = df[\"CLASS\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "219396b9-87cb-40d2-9dea-3d47d117cb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 0.4, 'c': 0.4, 'a': 0.2}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct answers: A, B and C. The last alternative (D) does not generate\n",
    "# a dictionary, but this can be fixed, e.g., by adding \".to_dict()\" at the end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
