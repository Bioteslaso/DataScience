{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6462427d-f93a-46a3-aef1-f16e7a7f7bbd",
   "metadata": {},
   "source": [
    "# Questions on Decision Trees and Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0c2c3-3e22-4597-af3a-71beca5eede0",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee0b18-14aa-477b-bbf2-fb5c03b7abec",
   "metadata": {},
   "source": [
    "Which of the following statements are correct regarding decision trees generated from categorical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3f7d0-cae5-4ec6-8f3f-487203568c54",
   "metadata": {},
   "source": [
    "A: In a tree with multi-way splits, the same feature may appear multiple times on a path from the root to a leaf\n",
    "\n",
    "B: In a tree with binary splits, the same feature may appear multiple times on a path from the root to a leaf\n",
    "\n",
    "C: A tree with multi-way splits will always contain more nodes than a tree with binary splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa350e1-6fde-40a1-9653-bfbcee99adf3",
   "metadata": {},
   "source": [
    "Correct answer: B\n",
    "\n",
    "Explanation: All instances that follow a path that contain an equality condition for a feature will have the same value for that feature, and hence splitting on that feature, will lead to that all instances fall into the same partition. (This will be the case even if some of the instances miss the feature value.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e7087-3555-4536-b621-0515ed07b2cb",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd8e3c-8e2f-465d-b1f3-6d6bd466def0",
   "metadata": {},
   "source": [
    "Which of the following statements are correct regarding decision trees with binary splits generated from numerical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f08207-06e2-4dbf-aa35-3845df086260",
   "metadata": {},
   "source": [
    "A: The same feature may appear multiple times on a path from the root to a leaf\n",
    "\n",
    "B: Missing values must be imputed prior to generating the tree\n",
    "\n",
    "C: Min-max normalization may have an effect on how the instances are partitioned "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638440a-1c5f-4ef8-a5c0-8e6b6605e12f",
   "metadata": {},
   "source": [
    "Correct answer: A\n",
    "\n",
    "Explanation: Instances with missing values may be distributed over the two child nodes following each split. Min-max normalization does not change the ordering of the instances and hence the number of possible splits and the resulting partitions stay the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623cc6c-e3ed-432b-b6a8-43ece77d34c9",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730b20b-5366-4ba5-a5f4-82e6171522f0",
   "metadata": {},
   "source": [
    "Assume that trees with multi-way splits are considered and that we by mistake have included a unique keyword identifier as a feature in the dataset.\n",
    "\n",
    "\n",
    "Which of the following statements are correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab7f50-f79d-43aa-9906-960c4dbda555",
   "metadata": {},
   "source": [
    "A Most likely, more informative features will be chosen for the splits \n",
    "\n",
    "B The feature will be evaluated as uninformative by Information gain\n",
    "\n",
    "C The feature will be evaluated as uninformative by Gini index\n",
    "\n",
    "D Laplace correction would not have an impact on whether the feature will be selected or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab669457-d1fd-421f-bfd9-f059b4122d4d",
   "metadata": {},
   "source": [
    "Correct answer: None\n",
    "\n",
    "Explanation: The relative frequency will be one for one of the classes and zero for the others, since there will be only one instance in each resulting child node. This leads to that both Information gain and Gini index will consider the split to be this most informative possible; there cannot be a better split according to the metrics. If the class probabilities are corrected by Laplace, the resulting probabilities will be 2/3 and 1/3, rather than 1/1 and 0/2 for binary classification tasks, leading to that the split will be far from perfect according to the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee2962f-dab7-4ccc-a1cd-8d613af132a2",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1224e-7e5d-4544-827d-f623354782cb",
   "metadata": {},
   "source": [
    "Assume that missing values are handled during generation and application of trees with binary splits.\n",
    "\n",
    "Which of the following statements are correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5522d4-1135-444a-8e9e-25870ad076f9",
   "metadata": {},
   "source": [
    "A: For categorical features, missing values must not be treated like non-missing values\n",
    "\n",
    "B: For numerical features, missing values must not be treated like non-missing values\n",
    "\n",
    "C: For test instances, we need to impute values before applying the tree\n",
    "\n",
    "D: If not imputing values before application, a test instance may follow a path for which the conditions are mutually exclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7705aa-bb38-41ec-ba8f-11f4be19c71d",
   "metadata": {},
   "source": [
    "Correct answer: B\n",
    "\n",
    "Explanation: A missing value may in principle be treated as a unique categorical value, allowing the (in)equality test to be well-defined. However, the corresponding test for numerical values cannot be defined for missing values in a reasonable way. We may distribute test instances with missing values over multiple nodes and aggregate the (multiple) leaf node predictions. The only way in which we can obtain a path with mutually exclusive conditions would be if we would allow the tree to be grown from empty partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2adaf2-2a35-4419-8be8-bceda2bfcc5d",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d45b3-a1a1-431f-ae2f-4106f5975147",
   "metadata": {},
   "source": [
    "Which of the following statements are correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e353271-fc0b-4313-a910-ddecbe4d3658",
   "metadata": {},
   "source": [
    "A: A strategy is needed to handle non-covered instances for decision lists\n",
    "\n",
    "B: A strategy is needed to handle conflicting rules for decision lists\n",
    "\n",
    "C: Rule sets are more compact than decision lists\n",
    "\n",
    "D: A decision tree can be directly converted to both a rule set and a decision list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e480c-7e73-40d7-80f0-1c93bffbe1d1",
   "metadata": {},
   "source": [
    "Correct answer: D\n",
    "\n",
    "Explanation: Only the first rule in a decision list will be applied and the last rule will cover any instances not covered by previous rules. Rule sets contain rules for all classes and are hence typically less compact than decision lists. Each path in a decision tree defines a rule and all the paths can be put in a set of (non-overlapping) rules. We could equally well form a decision list by replacing all rules predicting a specific class with a default rule, which is placed at the end of the decision list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c5d30-0007-4b65-a251-0f27ae52bcc3",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc13c55-b6b1-4efe-af78-80c32e393af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Which of the following parameter settings may lead to a larger number of nodes \n",
    "# in the resulting tree compared to the default?\n",
    "\n",
    "# A\n",
    "dt = tree.DecisionTreeClassifier(max_depth=3)\n",
    "# default=None -> np.inf\n",
    "\n",
    "# B\n",
    "dt = tree.DecisionTreeClassifier(min_samples_split=3)\n",
    "# default=2\n",
    "\n",
    "# C\n",
    "dt = tree.DecisionTreeClassifier(min_samples_leaf=3)\n",
    "# default=1\n",
    "\n",
    "# D\n",
    "dt = tree.DecisionTreeClassifier(max_features=\"log2\")\n",
    "# default=None -> n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d308f-488f-4d1d-a3f9-616e546e48a4",
   "metadata": {},
   "source": [
    "Correct answers: C, D\n",
    "\n",
    "Explanation: The first two options limit the depth of the tree in two different ways and hence cannot lead to an increased number of nodes (decisions regarding splits above the nodes that meet the termination conditions are not affected). The two last alternatives restrict what splits may be chosen, which may lead to that splits that would result in a more compact tree are left out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
