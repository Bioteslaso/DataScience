{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6462427d-f93a-46a3-aef1-f16e7a7f7bbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Questions on Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e7087-3555-4536-b621-0515ed07b2cb",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd8e3c-8e2f-465d-b1f3-6d6bd466def0",
   "metadata": {},
   "source": [
    "Assume that we have a dataset that we would like to use for model generation with a specific learning algorithm as well as for measuring the error. If we decide to use ten-fold cross validation, what of the following are correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f08207-06e2-4dbf-aa35-3845df086260",
   "metadata": {},
   "source": [
    "A: Each instance in the dataset will be used exactly once for testing.\n",
    "\n",
    "B: The performance estimate obtained from averaging the results on the ten test sets is an unbiased estimate of the performance of the model trained from the whole dataset. \n",
    "\n",
    "C: The setup corresponds to drawing ten independent samples of training and test sets from some unknown underlying distribution, where the training sets are nine times larger than the test sets. \n",
    "\n",
    "D: Using the t-distribution, we can generate a confidence interval from the ten measurements, which contains the true error level at the specified confidence level, if we assume that the errors are distributed normally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638440a-1c5f-4ef8-a5c0-8e6b6605e12f",
   "metadata": {},
   "source": [
    "Correct answer: A\n",
    "\n",
    "Explanation: Since the dataset is divided into ten (non-overlapping) partitions, each instance will appear in exactly one of them, and hence be used exactly once for testing. Since a model trained on 9/10 of the data will be evaluated on each test set, one can expect it to be (slightly) weaker than a model trained from the full set; the estimate can hence be expected to underestimate the performance of the latter. The ten sets of training and test sets are not independent; the same instance can not appear in multiple test sets and for each pair of training sets, there will be exactly an overlap of 8/9 of the instances. Since the observed measurements do not come from ten independent samples, the procedure to form confidence intervals come with no guarantees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623cc6c-e3ed-432b-b6a8-43ece77d34c9",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7bba7-281f-4a53-9323-2977524057dc",
   "metadata": {},
   "source": [
    "Assume that we are organizing a Kaggle competition and have received 100 contributions (by independent teams), in the form of predictive models generated\n",
    "from 1000 training instances. We have evaluated the models on a test set of\n",
    "the same size, which has been hidden from the teams, and found that the best\n",
    "performing model received an accuracy of 90.1%, which is just above the 90%\n",
    "threshold, which was a requirement for receiving an award of 100 000 USD. What should\n",
    "we expect when evaluating the best performing (selected) model on a second test set of the same size, assuming it has been sampled from the same underlying distribution as the first test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c020733-762b-43bd-9dec-8feed3c2d93b",
   "metadata": {},
   "source": [
    "A: If the competing models are clearly outperformed, then it is more likely that the selected model reaches the target level, compared to if most models performed on the same level.\n",
    "\n",
    "B: The chance that the selected model performs above the target level decreases if the winning team has submitted several contributions which differ randomly in 2% of the predictions.\n",
    "\n",
    "C: The chance that the selected model performs above the target level increases with the number of contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f566c0f5-aaf2-425e-acc6-eb813a6e9114",
   "metadata": {},
   "source": [
    "Correct answer: A, B, possibly also C\n",
    "\n",
    "Explanation: The estimated accuracy of a model will vary depending on what set of instances is used to evaluate its performance. If we consider such a set of instances to be a random sample drawn from some (unknown) target distribution, the estimated\n",
    "accuracy of the model will sometimes be lower and sometimes higher than the\n",
    "true accuracy (the accuracy of the model wrt. the target distribution). If the\n",
    "compared models have similar true accuracies, then the observed differences in\n",
    "the estimated accuracies will mainly be due to the sample we have drawn, and\n",
    "hence the extreme values of the observed estimated accuracies will be biased; the\n",
    "lowest value will be overly pessimistic and the highest value overly optimistic,\n",
    "and the difference between the extreme values and the true accuracies will increase with the number of compared models. Hence, if we select the highest\n",
    "of these values as an estimate for the true accuracy of the best model, we will\n",
    "systematically be overestimating the performance. However, if the true accuracy of one of the models is much higher than for the others (and hence would\n",
    "almost always be outperforming the others independently of the sample used),\n",
    "then this bias will be smaller. Since we typically do not beforehand know what\n",
    "the true accuracies are, there is hence a high risk that the estimated accuracy\n",
    "is indeed too high. In principle, an increased number of contributions could lead to that a model on a new, high level is found, hence eliminating the bias due to the sampling error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb05760-5615-49e2-887c-444e274b488c",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730b20b-5366-4ba5-a5f4-82e6171522f0",
   "metadata": {},
   "source": [
    "Assume that we want to compare a new algorithm to a baseline algorithm, for\n",
    "some classification task. As we are not sure what hyper-parameter settings to\n",
    "use for the new algorithm, we will investigate 100 different settings for that,\n",
    "while we use a standard hyper-parameter setting for the baseline algorithm.\n",
    "We first randomly split a given dataset into two equal-sized halves; one for\n",
    "model building and one for testing. We then employ 10-fold cross-validation\n",
    "using the first half of the data, measuring the accuracy of each model generated from an algorithm and hyper-parameter setting. Assume that the best performing hyper-parameter setting for the new algorithm results in a higher (cross-validation) accuracy than the baseline algorithm. Assume further that the two models (trained on the entire first half) are evaluated on the second half of the data. What of the following are correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab7f50-f79d-43aa-9906-960c4dbda555",
   "metadata": {},
   "source": [
    "A: We should expect to see the same relative performance, i.e., the new algorithm (with the best-performing hyper-parameter setting) outperforms the baseline (with the standard hyperparameter setting).\n",
    "\n",
    "B: If a majority of the evaluated hyperparameter settings lead to that the baseline is outperformed on the first half, we may expect the best performing configuration to outperform the baseline on the second half.\n",
    "\n",
    "C: The observed differences in performance between the different hyperparameter settings should not affect any conclusions on what relative performance to expect between the new and the baseline algorithm on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab669457-d1fd-421f-bfd9-f059b4122d4d",
   "metadata": {},
   "source": [
    "Correct answer: B\n",
    "\n",
    "Explanation: Since we have evaluated 100 different configurations for the novel algorithm, the\n",
    "observed (cross-validation) accuracy for the best performing of these is most\n",
    "likely biased, i.e., the performance is over-estimated, due to sampling error.\n",
    "Hence, although the best performing configuration outperforms the baseline\n",
    "when performing cross-validation on the first half of the data, the corresponding\n",
    "model trained with this configuration on this half and evaluated on the second\n",
    "half may very well be outperformed by the baseline. However, if a majority of\n",
    "the evaluated configurations outperform the baseline on the first half, we may\n",
    "expect the best performing configuration to still outperform the baseline on the\n",
    "second half. Moreover, if one (or a few) of the configurations clearly outperform the others, then we can expect the sampling error to be reduced, which may affect what should be expected from the comparison on the second half."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee2962f-dab7-4ccc-a1cd-8d613af132a2",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1224e-7e5d-4544-827d-f623354782cb",
   "metadata": {},
   "source": [
    "Assume that we would like to mitigate the curse-of-dimensionality for the decision tree\n",
    "learning algorithm and a very large dataset. We decide to select\n",
    "the ten features in the dataset with the highest information gain, out of the five hundred available categorical features. Assume that we now employ leave-one-out cross-validation on the reduced dataset, i.e., using only the ten selected features, together with the learning algorithm. What is correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5522d4-1135-444a-8e9e-25870ad076f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "A: The chances of generating a decision tree that perfectly fits the training data is reduced.\n",
    "\n",
    "B: Feature selection reduces the risk that the estimated performance is higher than what would be expected on an independent test set.\n",
    "\n",
    "C: If we would have kept all features, then the performance estimate would be less biased. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7705aa-bb38-41ec-ba8f-11f4be19c71d",
   "metadata": {},
   "source": [
    "Correct answer: A, C\n",
    "\n",
    "Explanation: Since we no longer have access to all features, the chance that two instances with different class labels end up in the same leaf, and consequently the probability of not perfectly fitting the training instances, increases. \n",
    "\n",
    "In the proposed setup, we would have access to the test labels when performing\n",
    "feature selection. This means that we could end up with a set of features which\n",
    "perform very well together with the learning algorithm on the test set, but which\n",
    "may not have been selected when basing the decision on what features to include\n",
    "using the training instances only. Since we have selected a model guided by the\n",
    "test instances, there is a risk that the estimated performance using these test\n",
    "instances is biased compared to when evaluating the model on independent test\n",
    "instances, which have neither been used for feature selection or model building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2adaf2-2a35-4419-8be8-bceda2bfcc5d",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d45b3-a1a1-431f-ae2f-4106f5975147",
   "metadata": {},
   "source": [
    "Assume that we want to develop a model and estimate its performance on\n",
    "independent data. We have therefore decided to randomly split an available\n",
    "dataset, with numerical features only, into a training and test set, using the former to train the model and the latter to estimate its performance. However, since the learning algorithm\n",
    "that we would like to use cannot directly deal with missing values, we have\n",
    "decided to employ some imputation technique prior to applying the algorithm.\n",
    "\n",
    "Which of the following statements are correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e353271-fc0b-4313-a910-ddecbe4d3658",
   "metadata": {},
   "source": [
    "A: The performance estimate will not be biased as long as imputation is done in the same way for the training and test sets.\n",
    "\n",
    "B: The performance estimate will not be biased as long as the test instances do not affect the way in which imputation is done.\n",
    "\n",
    "C: The performance estimate will not be biased if we replace all missing values with zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e480c-7e73-40d7-80f0-1c93bffbe1d1",
   "metadata": {},
   "source": [
    "Correct answer: B, C\n",
    "\n",
    "Explanation: To obtain an estimate of the performance on independent data, we need to make sure that no information from the test set is carried over to the trained model. However, if we employ imputation before splitting the dataset into a training and a test set, some information from the test set may be used when imputing feature values in the training set. This could potentially lead to that the model is better fitted to the test set than to some other independent test set, from\n",
    "which no information has been extracted. Hence, handling the training and test set in the same way may lead to a biased estimate. If the imputation is not affected by the test instances, it means that we should not expect the performance on the latter to be any different from independent instances. The same holds if we just replace missing instances with zero, as this is just one way of avoiding the test instances to affect the way in which imputation is done.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c5d30-0007-4b65-a251-0f27ae52bcc3",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8cbb6-5017-4e36-90ac-1235523bc169",
   "metadata": {},
   "source": [
    "Assume that you have a dataset with extreme (binary) class imbalance, making it hard to find a more accurate model than a dummy model that just predicts the majority class. Assume that we have decided to use a naive Bayes classifier. \n",
    "\n",
    "What of the following statements are correct?\n",
    "\n",
    "A: Random undersampling of the majority class can be expected to improve accuracy.\n",
    "\n",
    "B: Random oversampling of the minority class can be expected to improve accuracy.\n",
    "\n",
    "C: The effect of oversampling by including multiple copies of the minority class instances can be achieved by modifying the class priors.\n",
    "\n",
    "D: Random under- or oversampling can be expected to improve the area under the ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3356091a-265b-498f-9349-26659051ee8d",
   "metadata": {},
   "source": [
    "Correct answers: C\n",
    "\n",
    "Explanation: Random under- and oversampling will mainly affect the class priors and since these no longer will reflect what can be expected in independent test data, one would expect the accuracy to decrease. This will also be a consequence from that the class-conditional probabilities will be less precise for the undersampled class; the number of observations will be reduced. The same effect from adding copies of the minority class samples can achieved by modifying the class priors; the class-conditional probabilities will generally not be affected, as long as the probabilities for feature-value combinations with zero observations remain the same. Since random under- and oversampling (as well as changing the class priors) will not affect the rankings, the area under ROC will not be affected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
