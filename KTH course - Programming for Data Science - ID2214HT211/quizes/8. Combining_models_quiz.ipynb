{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6462427d-f93a-46a3-aef1-f16e7a7f7bbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Questions on Combining Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0c2c3-3e22-4597-af3a-71beca5eede0",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee0b18-14aa-477b-bbf2-fb5c03b7abec",
   "metadata": {},
   "source": [
    "Can the generation of a random forest with 1024 trees be at least as fast as the generation of a single decision tree (using the standard divide-and-conquer algorithm)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3f7d0-cae5-4ec6-8f3f-487203568c54",
   "metadata": {},
   "source": [
    "A: Yes, but only if the trees in the forest are generated in parallel\n",
    "\n",
    "B: Yes, if the number of training instances is large enough\n",
    "\n",
    "C: Yes, if the number of features is large enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa350e1-6fde-40a1-9653-bfbcee99adf3",
   "metadata": {},
   "source": [
    "Correct answer: C\n",
    "\n",
    "When generating a random forest, only a random subset of the features are evaluated when searching\n",
    "for the best way to split a node in a tree. If this subset constitutes less than one percent of the\n",
    "original features, then the cost of growing hundred trees is expected to be less than growing a single\n",
    "tree while evaluating all features (as the computational cost is directly proportional to the number\n",
    "of evaluated features). Moreover, as the trees in the random forests are generated from bootstrap\n",
    "replicates, the full-grown trees can be expected to be shallower, and hence faster to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e7087-3555-4536-b621-0515ed07b2cb",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd8e3c-8e2f-465d-b1f3-6d6bd466def0",
   "metadata": {},
   "source": [
    "Assume that we want to apply both random forests (RF) and the gradient boosting\n",
    "machine (GBM) to a regression task, where all regression values in the training set fall\n",
    "into the range $[0,1]$. Which of the following are correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f08207-06e2-4dbf-aa35-3845df086260",
   "metadata": {},
   "source": [
    "A: RF may produce predictions outside the range $[0,1]$\n",
    "\n",
    "B: GBM may produce predictions outside the range $[0,1]$\n",
    "\n",
    "C: Whether RF may produce predictions outside the range depends on the number of trees\n",
    "\n",
    "D: Whether GBM may produce predictions outside the range depends on the number of base models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638440a-1c5f-4ef8-a5c0-8e6b6605e12f",
   "metadata": {},
   "source": [
    "Correct answer: B, D\n",
    "\n",
    "Explanation: Random forests form their predictions by averaging the predictions of the individual trees, and since these (normally) are restricted to making predictions by averaging observed regression values in the leaves, they will not produce predictions that are higher (lower) than the highest (lowest) regression value in the training set. Gradient boosting machines form their predictions by summing the predictions of the individual trees, which may very well result in predictions that fall outside the range of observed regression values. However, if the GBM contains one base model only, it will predict the mean of the regression values in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623cc6c-e3ed-432b-b6a8-43ece77d34c9",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730b20b-5366-4ba5-a5f4-82e6171522f0",
   "metadata": {},
   "source": [
    "Assume that we would like to generate an ensemble of Naive Bayes classifiers in a similar way to how random forests are trained.\n",
    "\n",
    "Which of the following statements are correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab7f50-f79d-43aa-9906-960c4dbda555",
   "metadata": {},
   "source": [
    "A Bagging can be employed\n",
    "\n",
    "B Random feature selection can be employed\n",
    "\n",
    "C The class priors may differ between the base models\n",
    "\n",
    "D Numerical features have to be discretized in the same way across the base models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab669457-d1fd-421f-bfd9-f059b4122d4d",
   "metadata": {},
   "source": [
    "Correct answer: A, B, C\n",
    "\n",
    "Explanation: If we would like to generate an ensemble of naı̈ve Bayes classifiers using the\n",
    "strategy of random forests, it would mean that we introduce diversity in two\n",
    "ways; by training each classifier from a bootstrap replicate (through bagging)\n",
    "and by considering a random subset of the features for each classifier. Bagging may result in that the class frequencies may differ and hence the class priors for the base models. The preprocessing steps may be performed independently for each base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee2962f-dab7-4ccc-a1cd-8d613af132a2",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1224e-7e5d-4544-827d-f623354782cb",
   "metadata": {},
   "source": [
    "Assume that we would like to compare AdaBoost using decision stumps and fully grown decision trees as base models, respectively.\n",
    "\n",
    "Which of the following statements are correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5522d4-1135-444a-8e9e-25870ad076f9",
   "metadata": {},
   "source": [
    "A: We can expect more base models to be generated when using stumps\n",
    "\n",
    "B: We can expect the accuracy to be higher when using fully grown trees compared to using stumps\n",
    "\n",
    "C: We can expect that using more base models leads to improved predictive performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7705aa-bb38-41ec-ba8f-11f4be19c71d",
   "metadata": {},
   "source": [
    "Correct answer: A, C\n",
    "\n",
    "Explanation: The AdaBoost algorithm terminates before the maximum number of models\n",
    "have been generated, if either the error on the training set exceeds 50% or the\n",
    "error is 0%. The latter may very well happen if we allow the decision tree to be\n",
    "fully grown as it can easily overfit the training set. In the worst case, this leads\n",
    "to that AdaBoost will only iterate once, producing a model consisting of a single,\n",
    "typically rather weak, decision tree. In contrast, it is often not possible to find\n",
    "a single decision stump that perfectly classifies all training instances, allowing\n",
    "AdaBoost to continue generating ensemble members until the maximum number\n",
    "of iterations is reached, which we can expect to achieve a higher predictive performance, as this increases with the number of included base models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2adaf2-2a35-4419-8be8-bceda2bfcc5d",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d45b3-a1a1-431f-ae2f-4106f5975147",
   "metadata": {},
   "source": [
    "Assume that we first train base models using a set of algorithms\n",
    "from a given dataset and then generate a stacked model from the output of\n",
    "the base models when given the same dataset as input.\n",
    "\n",
    "Which of the following statements are correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e353271-fc0b-4313-a910-ddecbe4d3658",
   "metadata": {},
   "source": [
    "A: We can expect the stacked model to outperform each of the individual models\n",
    "\n",
    "B: If one base model is overfitting the training set, this will be compensated for by having included other base models\n",
    "\n",
    "C: We can expect averaging of the base models to outperform the stacked model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e480c-7e73-40d7-80f0-1c93bffbe1d1",
   "metadata": {},
   "source": [
    "Correct answer: C\n",
    "\n",
    "Explanation: If any of the algorithms has a tendency to overfit the training instances, such\n",
    "as the decision tree learning algorithm, it means that the label predicted by\n",
    "the corresponding model on a training instance with high likelihood will be the\n",
    "same as the original label of the training instance. In the extreme case, the base\n",
    "model will perfectly classify all training instances. In such a case, there is a risk\n",
    "that the stacking model will rely only on the output of such an overfitted base\n",
    "model, and hence not performning any better than this. Averaging can handle the problem unless a majority of the base models suffer from overfitting. The problem would be\n",
    "avoided by using a separate dataset and generate the stacking model from the\n",
    "output of the base models on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c5d30-0007-4b65-a251-0f27ae52bcc3",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5251e9a6-982e-4e51-a319-5b45b1f03dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Now write:\n",
    "# rng.choice([1,2,3],10)\n",
    "# instead of (legacy):\n",
    "# np.random.choice([1,2,3],10)\n",
    "\n",
    "n = 10\n",
    "d = 5\n",
    "X = rng.random((n,d))\n",
    "y = rng.choice([0,1], n)\n",
    "\n",
    "# Which of the following results in a proper \n",
    "# bootstrap replicate of the objects and labels:\n",
    "\n",
    "# A\n",
    "X_bootstrap = X[rng.choice(n,n,replace=False)]\n",
    "y_bootstrap = y[rng.choice(n,n,replace=False)]\n",
    "\n",
    "# B\n",
    "X_bootstrap = X[rng.choice(n,n,replace=True)]\n",
    "y_bootstrap = y[rng.choice(n,n,replace=True)]\n",
    "\n",
    "# C\n",
    "bootstrap = rng.choice(n,n,replace=False)\n",
    "X_bootstrap = X[bootstrap]\n",
    "y_bootstrap = y[bootstrap]\n",
    "\n",
    "# D\n",
    "bootstrap = rng.choice(n,n,replace=True)\n",
    "X_bootstrap = X[bootstrap]\n",
    "y_bootstrap = y[bootstrap]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3356091a-265b-498f-9349-26659051ee8d",
   "metadata": {},
   "source": [
    "Correct answers: D\n",
    "\n",
    "Explanation: The random selection should be with replacement and the same indexes should be used for both the objects and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aefd95f0-730b-43f8-a2ef-e3de97f7f403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One more illustration:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "rng.integers(0,2,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
